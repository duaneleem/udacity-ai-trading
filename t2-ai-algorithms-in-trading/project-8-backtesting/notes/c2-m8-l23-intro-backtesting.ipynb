{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6c0c8d-d4bb-4a1f-97c0-d9c40d63b308",
   "metadata": {},
   "source": [
    "# AI for Trading | Module 8 | L23: Intro to Backtesting\n",
    "## 1. Intro\n",
    "https://youtu.be/RjUIPy_robk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e48ccc-5949-42b1-887c-97fe54a2aa7a",
   "metadata": {},
   "source": [
    "## 2. What is a Backtest?\n",
    "- https://youtu.be/q2dW6-ZRaXE\n",
    "\n",
    "\n",
    "## 3. Backtest Validity\n",
    "- https://youtu.be/KTlrel9p6Q0\n",
    "\n",
    "### A \"valid\" backtest must satisfy:\n",
    "1. The profit calculation is realistic.\n",
    "2. No lookahead bias.\n",
    "\n",
    "### Examples of (1) unrealistic profit calculation:\n",
    "- Underestimating trading costs\n",
    "- ignoring categories of costs, such as financing or taxes\\\n",
    "- unrealistic volumes\n",
    "- executing at the close price\n",
    "- unrealistic borrowing\n",
    "\n",
    "### Examples of (2) lookahead bias:\n",
    "- use of \"tomorrow's news today\"\n",
    "- use of \"this evening's news today\"\n",
    "- use of today's closing price for trading today\n",
    "\n",
    "\n",
    "## 4. Backtest Overfitting\n",
    "- https://youtu.be/MnWHGIiqjns\n",
    "\n",
    "You can access Elements of Statistical Learning by Hastie, Tibshirani and Friedman [here](https://web.stanford.edu/~hastie/Papers/ESLII.pdf).\n",
    "\n",
    "[This website](http://datagrid.lbl.gov/backtest/index.php) is useful for exploring backtest overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32442833-3f9c-4f87-bead-99c674b568e8",
   "metadata": {},
   "source": [
    "## 5. Overtrading\n",
    "- https://youtu.be/0cdGLRDI_Sk\n",
    "\n",
    "Trading in larger sizes than would be optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccda1a-de93-457c-9092-b6d1df0fca89",
   "metadata": {},
   "source": [
    "## 6. Backtest Best Practices\n",
    "\n",
    "1. Use cross-validation to achieve just the right amount of model complexity.\n",
    "2. Always keep an out-of-sample test dataset. You should only look at the results of a test on this dataset once all model decisions have been made. If you let the results of this test influence decisions made about the model, you no longer have an estimate of generalization error.\n",
    "3. Be wary of creating multiple model configurations. If the Sharpe ratio of a backtest is 2, but there are 10 model configurations, this is a kind of multiple comparison bias. This is different than repeatedly tweaking the parameters to get a sharpe ratio of 2.\n",
    "4. Be careful about your choice of time period for validation and testing. Be sure that the test period is not special in any way.\n",
    "5. Be careful about how often you touch the data. You should only use the test data once, when your validation process is finished and your model is fully built. Too many tweaks in response to tests on validation data are likely to cause the model to increasingly fit the validation data.\n",
    "6. Keep track of the dates on which modifications to the model were made, so that you know the date on which a provable out-of-sample period commenced. If a model hasn’t changed for 3 years, then the performance on the past 3 years is a measure of out-of-sample performance.\n",
    "\n",
    "Traditional ML is about fitting a model until it works. Finance is different—you can’t keep adjusting parameters to get a desired result. Maximizing the in-sample sharpe ratio is not good—it would probably make out of sample sharpe ratio worse. It’s very important to follow good research practices.\n",
    "\n",
    "\n",
    "## 7. Structural Changes\n",
    "- https://youtu.be/EaxepBSycbQ\n",
    "\n",
    "Structural changes could mean there is a change in the industry or a regulation that would affect the market.\n",
    "\n",
    "\n",
    "## 8. Gradient Boosting\n",
    "In our exercise about overfitting, we're going to use a type of model that we haven't yet encountered in the course, but that's popular and well-known, and has been used successfully in machine learning competitions: gradient boosted trees. Here we're going to give you a short introduction to gradient boosting so that you have an intuition for how the model works.\n",
    "\n",
    "We've already studied ensembling; well, boosting is another type of ensembling, or combining weak learners into a strong learner. It's also typically done with decision trees as the weak learners. The video below will give you a quick introduction to boosting by telling you about the first successful boosting algorithm, Adaboost.\n",
    "\n",
    "https://youtu.be/HD6SRBWKGUE\n",
    "\n",
    "This video only scratched the surface, but you can see that Adaboost basically works in the following way:\n",
    "\n",
    "- Fit an additive model (ensemble) in a forward stage-wise manner.\n",
    "- In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.\n",
    "- In Adaboost, “shortcomings” are identified by high-weight datapoints (this is what is meant in the video by making the points \"bigger\").\n",
    "\n",
    "Gradient boosting is very similar. In essence, it allows the user to customize the method used to identify the \"shortcomings\" of existing weak learners (the **cost function**). If you want to learn more about the details, [try this page](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34674180-f211-439c-adee-ba0b4e4c0aa9",
   "metadata": {},
   "source": [
    "## 10. AI in Finance Interview\n",
    "In this video, we spoke with Gordon to learn a little more about his experiences working with AI in finance.\n",
    "\n",
    "https://youtu.be/vqHsk3hjBLU\n",
    "\n",
    "\n",
    "## 11. Outro\n",
    "https://youtu.be/mFk_HYJLF1w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UdacityAI (3.6)",
   "language": "python",
   "name": "udacityai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
